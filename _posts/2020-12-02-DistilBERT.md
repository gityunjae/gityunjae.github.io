---
category: papers
layout: post
title: DistilBERT
---

제목: DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

저자: Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf

발행년도: 2020

DistilBERT는 Hugging Face에서 발표한 pre-train language model로 기존 BERT보다 크기가 40% 작지만 언어 이해 능력은 기존의 97%를 유지하면서 60% 더 빠른 모델이다.

## 1. Introduction

## 2. Knowledge distillation

## 3. DistilBERT: a distilled version of BERT

## 4. Experiments

## 5. Related work

## 6. Conclusion and future work
