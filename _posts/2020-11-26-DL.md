---
category: DLfS
layout: post
title: 8. 딥러닝
---

딥러닝은 계층을 깊게 한 심층신경망이다.

딥러닝의 정확도 향상에 기여하는 것을로는 앙상블 모델 사용, 학습률 감소, 데이터 확장 등이 있다.

이 중 데이터 확장은 입력 이미지를 인위적으로 확장해서 데이터를 늘리는 방법인데, 회전이나 이동, 자르기, 반전, 밝기 변화, 스케일 변화 등이 해당한다. (이 중 반전의 경우 이미지의 대칭성을 고려하지 않는 task에서만 사용된다.)

층을 깊게 하는 이유에 대한 이론적인 근거가 있는 것은 아니지만 대규모 이미지 인식 대회 결과를 보면 상위 모델들이 모두 딥러닝이고 신경망을 더 깊이 쌓는 경향도 있다.

또한 층을 깊이 하면 매개변수의 갯수가 적어지는데, 다시 말해 더 적은 매개변수로 같은 수준의 표현력을 얻을 수 있다.

(img1)

작은 필터를 여러변 겹쳐 신경망을 깊게 하면 적은 매개변수로 더 넓은 수용 영역을 처리할 수 있고, 층마다 ReLU등의 활성화 함수 사용으로 표현력이 더 좋아진다.

층을 깊게하면 효율도 더 좋은데, 더 적은 학습 데이터로 더 빠르게 학습할 수 있다고 한다.

예를 들어서 개를 인식한다고 하면 얕은 신경망은 한번에 많은 특징을 이해해야 해서 더 많은 데이터를 대상으로 더 많이 학습해야 하지만 깊은 신경망은 학습할 내용을 계층적으로 분해해서 각 층이 학습해야 할 문제를 풀기 쉬운 간단한 문제로 쪼갤 수 있다.

<br>
이미지넷은 100만장이 넘는 이미지를 담고 있는 데이터셋이고, 각 데이터는 이미지와 레이블 쌍으로 이루어져 있다.

ILSVRC(ImageNet Large Scale Visual Recognition Challenge)라고 이미지 인식 기술을 겨루는 대회가 있는데, 2012년 AlexNet이 등장해서 엄청난 성능으로 주목을 받았다.
이후 딥러닝이 다시 흥하기 시작했는데, 시험항목 중 분류(classification)에서 2012년 이후 항상 선두가 딥러닝 모델이었고, 2015년 나온 ResNet이라는 모델은 인간의 인식 능력을 넘어설 정도였다.


VGG는 구성이 간단한 기본적인 CNN 구조지만 계층이 16층(또는 19층)인 모델이다.

(img)

3x3의 작은 필터를 사용한 합성곱 계층을 연속으로 거치고, 2~4회 연속으로 풀링 계층을 사용해서 크기를 절반씩 줄이고, 마지막에는 완전연결계층을 통과해서 결과를 출력한다.

GoogLeNet은 인셉션 구조라는 것을 사용하는데 인셉션 구조에서는 크기가 다른 필터와 풀링을 적용한 결과를 결합해서 사용한다.

(img)

(img)

또한 1x1 크기의 필터를 사용하는데, 이를 통해 채널쪽 크기를 줄일 수 있다고 한다.(이 부분은 잘 이해가 안된다)

ResNet은 마이크로소프트 팀이 개발한 네트워크로 스킵연결기법을 사용해서 층의 깊이에 비례해 성능을 향상시킨다.

(img)

스킵연결을 사용하면 
ResNet은 마이크로소프트 팀이 개발한 네트워크로 스킵연결기법을 사용해서 층의 깊이에 비례해 성능을
