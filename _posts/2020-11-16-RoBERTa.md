---
category: papers
layout: post
title: RoBERTa
---
제목: RoBERTa: A Robustly Optimized BERT Pretraining Approach

저자: Y.Liu et al (UW with Facebook AI)

발행년도: 2019

Language model pretraining을 통해 성능의 향상이 가능하지만, 다양한 접근법 사이의 비교를 하기에는 더 어려워졌다. 또한 학습에 들어가는 연산이 매우 많기 때문에 hyperparameter가 최종 결과에 큰 영향을 미친다.

이 논문에서는 BERT pretraining을 재현하는 시도 중 BERT가 매우 undertrained 되었고, 사실 BERT만 가지고도 이후에 나온 모든 모델들의 성능을 능가할 수 있다고 주장한다. 해당 모델은 GLUE, RACE, 그리고 SQuAD task에 대해 state-of-the-art 결과를 냈으며, 모델과 코드는 <a href="https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a> 에 올라와있다.

## Introduction
ELMo, GPT, BERT, XLM, 그리고 XLNet등의 셀프 트레이닝 방식은 눈에 띄는 성능 향상을 이뤘지만, 각각의 모델에서 어떤 요소들이 가장 기여도가 큰지를 결정하기는 어려울 수 있다. 학습을 시키는 데에 연산량이 많이 들기 때문에 튜닝의 횟수도 제한되고, 각 모델별로 다양한 크기의 개인 비공개 데이터를 사용하여 학습을 하기 때문에 modeling advance의 영향을 측정하는 것이 제한된다.

이 연구자들은 BERT를 재현해보다가 BERT가 매우 undertrained 되었다고 판단하여 <b>BERT 모델을 학습시키는 더 개선된 방식을 적용하는 RoBERTa를 제안한다.</b>

RoBERTa가 BERT와 다른 점은 모델을 더 많은 데이터에 대해 더 오래, 더 큰 배치로 학습을 시켰고, next sentence prediction task를 없애고, 더 긴 sequences에 대해 학습하였으며 학습 데이터에 대한 마스킹 패턴이 다이나믹하게 바뀔 수 있게 했다.

또한 CC_NEWS라는 다른 비공개적으로 사용된 데이터셋과 비슷한 크기의 큰 데이터셋을 새로 수집하였고 이를 통해 학습 데이터 크기의 영향을 확인할 수 있었다.

모델을 학습시킨 결과 9개의 GLUE task중 4개 task에 SoTA를 찍었고 SQuAD와 RACE에서도 SoTA를 찍을 수 있었다.

이 논문의 Contribution은 다음과 같다.

> 1. 학습 전략과 BERT design choices를 제안하고 downstream task에 더 좋은 성능을 낼 수 있는 대안 또한 제시한다. <br>
> 2. 새로운 데이터셋인 CC-NEWS 데이터셋을 사용하여 pretrain시 데이터를 더 많이 사용하면 downatream task에 적용했을때 추가적인 성능 향상을 이룰 수 있음을 보인다.<br>
> 3. Masked language model pretraining이 제대로 설계하면 최근에 나온 모델들과 경쟁할만한 성능이 나온다.

<br>
코드의 구현은 PyTorch를 사용하였다.

## Background
