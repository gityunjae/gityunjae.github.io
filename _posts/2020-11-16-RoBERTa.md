---
category: papers
layout: post
title: RoBERTa
---
제목: RoBERTa: A Robustly Optimized BERT Pretraining Approach

저자: Y.Liu et al (UW with Facebook AI)

발행년도: 2019

Language model pretraining을 통해 성능의 향상이 가능하지만, 다양한 접근법 사이의 비교를 하기에는 더 어려워졌다. 또한 학습에 들어가는 연산이 매우 많기 때문에 hyperparameter가 최종 결과에 큰 영향을 미친다.

이 논문에서는 BERT pretraining을 재현하는 시도 중 BERT가 매우 undertrained 되었고, 사실 BERT만 가지고도 이후에 나온 모든 모델들의 성능을 능가할 수 있다고 주장한다. 해당 모델은 GLUE, RACE, 그리고 SQuAD task에 대해 state-of-the-art 결과를 냈으며, 모델과 코드는 <a href="https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a> 에 올라와있다.

## 1. Introduction
ELMo, GPT, BERT, XLM, 그리고 XLNet등의 셀프 트레이닝 방식은 눈에 띄는 성능 향상을 이뤘지만, 각각의 모델에서 어떤 요소들이 가장 기여도가 큰지를 결정하기는 어려울 수 있다. 학습을 시키는 데에 연산량이 많이 들기 때문에 튜닝의 횟수도 제한되고, 각 모델별로 다양한 크기의 개인 비공개 데이터를 사용하여 학습을 하기 때문에 modeling advance의 영향을 측정하는 것이 제한된다.

이 연구자들은 BERT를 재현해보다가 BERT가 매우 undertrained 되었다고 판단하여 <b>BERT 모델을 학습시키는 더 개선된 방식을 적용하는 RoBERTa를 제안한다.</b>

RoBERTa가 BERT와 다른 점은 모델을 더 많은 데이터에 대해 더 오래, 더 큰 배치로 학습을 시켰고, next sentence prediction task를 없애고, 더 긴 sequences에 대해 학습하였으며 학습 데이터에 대한 마스킹 패턴이 다이나믹하게 바뀔 수 있게 했다.

또한 CC_NEWS라는 다른 비공개적으로 사용된 데이터셋과 비슷한 크기의 큰 데이터셋을 새로 수집하였고 이를 통해 학습 데이터 크기의 영향을 확인할 수 있었다.

모델을 학습시킨 결과 9개의 GLUE task중 4개 task에 SoTA를 찍었고 SQuAD와 RACE에서도 SoTA를 찍을 수 있었다.

이 논문의 Contribution은 다음과 같다.

1. 학습 전략과 BERT design choices를 제안하고 downstream task에 더 좋은 성능을 낼 수 있는 대안 또한 제시한다. <br>
2. 새로운 데이터셋인 CC-NEWS 데이터셋을 사용하여 pretrain시 데이터를 더 많이 사용하면 downatream task에 적용했을때 추가적인 성능 향상을 이룰 수 있음을 보인다.<br>
3. Masked language model pretraining이 제대로 설계하면 최근에 나온 모델들과 경쟁할만한 성능이 나온다.

코드의 구현은 PyTorch를 사용하였다.
<br>
## 2. Background
BERT의 pretraining 접근법과 학습 방법들에 대해 알아보자.

### 2.1 Setup
BERT에서 segment는 주로 하나 이상의 자연어 문장으로 이루어져 있는 sequence이다. 여기서 BERT는 두 segment를 [SEP]이라는 구분자를 끼고 concat한 것 앞에 [CLS], 뒤에 [EOS]를 붙인 것을 input으로 받는데 두 시퀀스의 길이의 합이 최대 길이를 넘지 않게 제약조건을 둔다.

모델은 먼저 unlabeled text corpus에 대해 pretrain 되고 이후 end-task labeled data로 finetuning한다.

### 2.2 Architecture
BERT는 transformer 구조를 사용하는데 레이어의 개수를 L, 셀프 어텐션 헤드의 개수를 A, 그리고 은닉 차원수(hidden dimension)를 H로 두고 사용하였다.

### 2.3 Training Objectives
BERT를 pretraining 할 때에는 masked language modeling과 next sentence prediction 두 가지 objective를 사용한다.

### Masked Language Model (MLM)
입력받은 시퀀스에서 토큰을 랜덤하게 뽑아 [MASK] 토큰으로 대체한다. 이 task의 objective는 masked token을 예측하는 cross-entropy를 줄이는 것이다. 

BERT에서는 15%의 토큰을 뽑아서 이 중 80%는 [MASK] 토큰으로, 10%는 그대로, 그리고 10%는 랜덤하게 선택된 단어로 대체한다. 원래 구현에서는 마스킹이 초반에 한번 수행되었지만 실제로는 데이터를 복제해서 매번 다른 토큰을 마스킹한다.

### Next Sentence Prediction (NSP)
이 task는 두 세그먼트가 원 텍스트에서 연속적으로 나오는지 예측하는 binary classification을 수행한다. 실제로 연속된 세그먼트인 경우와 연속이 아닌 세그먼트인 경우를 같은 확률로 샘플링한다. 

이 task는 문장 쌍의 관계를 추론하는 등의 downstream task에서의 성능을 향상시키기 위해 추가해준 것이다.

### 2.4 Optimization
BERT는 Adam optimizer를 사용하고 첫 만 스텝에 warm up을, 그리고 모든 레이어에 0.1의 dropout를 사용하고 GELU 활성화함수를 사용한다. 모델은 100만 update동안 학습되며 미니 배치는 최대 512 토큰의 256 시퀀스를 사용한다.

### 2.4 Data
BERT는 총 16GB의 BOOKCORPUS와 영문 위키피디아 데이터로 학습되었다.
<br>
## 3. Experimental Setup
### 3.1 Implementation
해당 논문의 저자들은 BERT를 FAIRSEP를 사용하고 재구현하였고, 최적화에 관련해서는 peak learning rate와 warmup steps를 제외하고는 원 BERT 구현과 동일한 hyperparameter를 사용하였다. 그 외에 상황에 따라 Adam optimizer를 살짝 tuning하거나  안정성을 위해 다른 값들도 조금 바꿔주었을 때 더 좋은 성능이 나오기도 하였다.

시퀀스의 최대 토큰 수는 512개로 지정하였고 full-length sequence만 사용하여 학습하였다.

### 3.2 Data
BERT 방식의 학습은 데이터 규모에 크게 영향을 받는다. 데이터 크기가 클수록 end-task 성능이 높아진다는 연구 결과도 있었다. 실제로 BERT를 더 크고 다양한 데이터로 학습시키려는 시도도 종종 있었는데, 데이터셋은 공개되지 않은 경우도 있었다.

이 연구를 위해 최대한 많은 데이터를 수집하였고, 사용한 데이터는 다음과 같다.

* BOOKCORPUS + English WIKIPEDIA: original BERT data (16GB) <br>
* CC-NEWS: CommonCrawl News dataset의 영어 부분 직접 발췌 (76GB) <br>
* OPENWEBTEXT: Web Text Corpus (38GB) <br>
* STORIES: CommonCrawl data에서 이야기 스타일의 데이터 (31GB) <br>

### 3.3 Evaluation
기존 BERT 연구와 동일하게 기학습 시킨 모델을 다음의 세 가지 benchmark를 사용하고 평가한다.

* GLUE (General Language Understanding Evaluation): 자연어 이해를 측정할 수 있는 9개의 데이터셋 집합으로 단일문장 분류 또는 문장쌍 분류 task로 이루어져있다. 학습 데이터와 검증 데이터 분할도 되어있고 제출 서버가 있어서 리더보드로 다른 모델들과 성능을 비교할 수도 있다. Section 4에서의 결과는 검증 데이터 기준이며 finetuning은 기존의 BERT와 동일하게 진행하였다. Section 5에서는 public leaderboard 기준의 성능 또한 첨부하였다. <br>
* SQuAD (Stanford Question Answering Dataset): 문맥을 표현하는 문단과 질문을 주면 문맥에서 정답에 해당하는 span을 추출하는 task이다. 이 데이터 셋은 문맥이 언제나 정담을 포함하고 있는 V1.1과 몇몇 문제는 문맥상에서 정답을 찾을 수 없는 V2.0 총 두 가지 버전을 모두 사용하였다. SQuAD V1.1에서는 BERT와 같은 방식을 사용하였고 SQuAD V2.0에서는 질문이 대답할 수 있는 질문인지를 판단하는 이진분류기를 추가해주었다. 그리고 평가시에는 분류 결과 정답이 있다고 분류되는 문제들에 대해서만 정답 span을 예측하도록 하였다. <br>
* RACE (ReAding Comprehension from Examinations): 28000 문단과 거의 10만 개의 질문으로 구성된 대규모의 독해 데이터셋이다. 이는 중국에서 시행되는 영어 시험으로 중고등생 대상이고, 각 문단은 하나 이상의 질문들에 대응된다. task는 4지선다에서 맞는 답 하나를 고르는 것이다. RACE는 다른 독해 데이터셋에 비해 문맥이 특히 더 길고 질문이 요구하는 추론의 정도가 매우 크다는 특징이 있다.<br>
<br>
## 4. Training Procedure Analysis
