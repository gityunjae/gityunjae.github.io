---
category: papers
layout: post
title: RoBERTa
---
제목: RoBERTa: A Robustly Optimized BERT Pretraining Approach

저자: Y.Liu et al (UW with Facebook AI)

발행년도: 2019

Language model pretraining을 통해 성능의 향상이 가능하지만, 다양한 접근법 사이의 비교를 하기에는 더 어려워졌다. 또한 학습에 들어가는 연산이 매우 많기 때문에 hyperparameter가 최종 결과에 큰 영향을 미친다.

이 논문에서는 BERT pretraining을 재현하는 시도 중 BERT가 매우 undertrained 되었고, 사실 BERT만 가지고도 이후에 나온 모든 모델들의 성능을 능가할 수 있다고 주장한다. 해당 모델은 GLUE, RACE, 그리고 SQuAD task에 대해 state-of-the-art 결과를 냈으며, 모델과 코드는 <a href="https://github.com/pytorch/fairseq">https://github.com/pytorch/fairseq</a> 에 올라와있다.

## Introduction
ELMo, GPT, BERT, XLM, 그리고 XLNet등의 셀프 트레이닝 방식은 눈에 띄는 성능 향상을 이뤘지만, 각각의 모델에서 어떤 요소들이 가장 기여도가 큰지를 결정하기는 어려울 수 있다. 학습을 시키는 데에 연산량이 많이 들기 때문에 튜닝의 횟수도 제한되고, 각 모델별로 다양한 크기의 개인 비공개 데이터를 사용하여 학습을 하기 때문에 modeling advance의 영향을 측정하는 것이 제한된다.

이 연구자들은 BERT를 재현해보다가 BERT가 매우 undertrained 되었다고 판단하여 <b>BERT 모델을 학습시키는 더 개선된 방식을 적용하는 RoBERTa를 제안한다.</b>

RoBERTa가 BERT와 다른 점은 모델을 더 많은 데이터에 대해 더 오래, 더 큰 배치로 학습을 시켰고, next sentence prediction task를 없애고, 더 긴 sequences에 대해 학습하였으며 학습 데이터에 대한 마스킹 패턴이 다이나믹하게 바뀔 수 있게 했다.

또한 CC_NEWS라는 다른 비공개적으로 사용된 데이터셋과 비슷한 크기의 큰 데이터셋을 새로 수집하였고 이를 통해 학습 데이터 크기의 영향을 확인할 수 있었다.

모델을 학습시킨 결과 9개의 GLUE task중 4개 task에 SoTA를 찍었고 SQuAD와 RACE에서도 SoTA를 찍을 수 있었다.

이 논문의 Contribution은 다음과 같다.

> 1. 학습 전략과 BERT design choices를 제안하고 downstream task에 더 좋은 성능을 낼 수 있는 대안 또한 제시한다. <br>
> 2. 새로운 데이터셋인 CC-NEWS 데이터셋을 사용하여 pretrain시 데이터를 더 많이 사용하면 downatream task에 적용했을때 추가적인 성능 향상을 이룰 수 있음을 보인다.<br>
> 3. Masked language model pretraining이 제대로 설계하면 최근에 나온 모델들과 경쟁할만한 성능이 나온다.

<br>
코드의 구현은 PyTorch를 사용하였다.

## Background
BERT의 pretraining 접근법과 학습 방법들에 대해 알아보자.

### 2.1 Setup
BERT에서 segment는 주로 하나 이상의 자연어 문장으로 이루어져 있는 sequence이다. 여기서 BERT는 두 segment를 [SEP]이라는 구분자를 끼고 concat한 것 앞에 [CLS], 뒤에 [EOS]를 붙인 것을 input으로 받는데 두 시퀀스의 길이의 합이 최대 길이를 넘지 않게 제약조건을 둔다.

모델은 먼저 unlabeled text corpus에 대해 pretrain 되고 이후 end-task labeled data로 finetuning한다.

### 2.2 Architecture
BERT는 transformer 구조를 사용하는데 레이어의 개수를 L, 셀프 어텐션 헤드의 개수를 A, 그리고 은닉 차원수(hidden dimension)를 H로 두고 사용하였다.

### 2.3 Training Objectives
BERT를 pretraining 할 때에는 masked language modeling과 next sentence prediction 두 가지 objective를 사용한다.

### Masked Language Model (MLM)
입력받은 시퀀스에서 토큰을 랜덤하게 뽑아 [MASK] 토큰으로 대체한다. 이 task의 objective는 masked token을 예측하는 cross-entropy를 줄이는 것이다. 

BERT에서는 15%의 토큰을 뽑아서 이 중 80%는 [MASK] 토큰으로, 10%는 그대로, 그리고 10%는 랜덤하게 선택된 단어로 대체한다. 원래 구현에서는 마스킹이 초반에 한번 수행되었지만 실제로는 데이터를 복제해서 매번 다른 토큰을 마스킹한다.

### Next Sentence Prediction (NSP)
이 task는 두 세그먼트가 원 텍스트에서 연속적으로 나오는지 예측하는 binary classification을 수행한다. 실제로 연속된 세그먼트인 경우와 연속이 아닌 세그먼트인 경우를 같은 확률로 샘플링한다. 

이 task는 문장 쌍의 관계를 추론하는 등의 downstream task에서의 성능을 향상시키기 위해 추가해준 것이다.

