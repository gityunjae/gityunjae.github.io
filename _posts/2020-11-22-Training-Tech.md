---
category: DLfS
layout: post
title: 6. 학습 관련 기술들
---
## 매개변수 갱신
신경망의 학습 목적은 손실 함수 값을 최대한 낮추는 매개변수를 찾는 것이다. 이는 다시 말하면 매개변수의 최적값을 찾는 것이고, 최적화(optimization)에 해당한다.

앞에서 미니 배치에 매개변수의 기울기로 미분을 사용하는 확률적 경사 하강법(SGD)을 알아보았다.

![01](https://gityunjae.github.io/images/Chap06/01.JPG)

SGD 방식은 단순하고 구현이 쉽지만 비효율적인 경우도 있다.
비등방성 함수(방향에 따라 성질, 기울기가 달라지는 함수)에서는 SGD의 탐색경로가 비효율적이다.

![02](https://gityunjae.github.io/images/Chap06/02.png)

이러한 단점을 개선해주는 방법으로 모멘텀, AdaGrad, Adam 방법이 있다.

### 모멘텀

![03](https://gityunjae.github.io/images/Chap06/03.JPG)

모멘텀은 v라는 속도에 해당하는 변수를 사용하는 방식으로 기울기 방향으로 힘을 받아 물체가 가속되는 물리 법칙을 적용한 방식이다.

![04](https://gityunjae.github.io/images/Chap06/04.png)

SGD와 비교해서 지그재그의 정도가 덜한것을 알 수 있는데, x축 방향 힘이 작지만 방향이 바뀌지 않기 때문에 점점 가속을 하기 때문이다.

### AdaGrad
신경망 학습에서 학습률은 너무 작으면 학습이 너무 오래 걸리고 너무 크면 값이 발산해버릴 수 있는, 상당히 중요한 파라미터이다.

학습률 감소(Learning rate decay)란 학습률을 크게 시작해서 점점 학습률을 줄여 나가는 방식인데, 기계학습에서 자주 쓰인다.

AdaGrad 방식에서는 학습률을 각각의 매개변수에 맞춤형으로 만들어주는 Adaptive 방식이다.

![05](https://gityunjae.github.io/images/Chap06/05.JPG)

식에서 h는 기존 기울기를 제곱해서 더해준 값이고, h의 제곱근의 역수를 곱해 학습률을 조정하는데, 매개변수 원소 중 많이 움직인 원소의 학습률이 낮아지는 방식이다.

![06](https://gityunjae.github.io/images/Chap06/06.png)

AdaGrad의 단점은 학습이 진행될수록 갱신강도가 약해져서 어느 순간부터 갱신이 안될 수 있다는 점인데, 이를 개선해서 지수이동평균 방식을 사용하여 과거의 기울이는 덜 반영하고 최근 정보는 더 크게 반영하는 방법을 RMSProp 방법이라고 한다.

### Adam
Adam 방식은 모멘텀과 AdaGrad를 융합한 모델로 효율적이고, 하이퍼 파라미터의 편향 보정 또한 진행된다고 한다. (논문을 보면 더 자세히 나온다고 하는데, 다음에 다른 교재를 공부할 때 기회가 되면 원리를 이해해보기로 하자.)

![07](https://gityunjae.github.io/images/Chap06/07.png)

SGD, 모멘텀, AdaGrad, Adam 모두 장단점이 있고, 각각의 문제에 따라, 하이퍼 파라미터에 따라 성능이 다르기 때문에 해결하려는 문제에 대해 여러가지로 시도해보는 것이 좋다.

## 가중치의 초기값
신경망 학습에서 또 중요한 것은 가중치의 초기값이다.

가중치 감소란 가중치 매개변수의 값이 작아지도록 학습하여 과적합을 방지하는 방법이다.
이 때 가중치가 작게 하기 위해 모든 가중치 값을 0으로 초기화하면 어떻게 될까?

가중치가 모두 0이면 전파시 같은 값들이 다음 계층으로 전파되기 때문에 갱신을 해도 값이 그대로이다.

이런 일을 방지하기 위해 초기값을 무작위로 설정해주는 것이 좋다.

은닉층의 활성화 값을 가지고 분포에서 중요한 정보를 얻을 수 있다.

예시로 활성화 함수로 시그모이드 함수를 사용하고 각 층의 뉴런이 100개인 5계층 신경망을 무작위로 초기화하여 입력 데이터를 1000개 입력해줄 때 표준편차가 1인 정규 분포 값으로 초기화 하는 경우 출력이 0과 1에 집중되는데, 출력이 0과 1에 가까워지면 미분값이 0에 가까워지는 시그모이드 함수의 특성 때문에 기울기값이 갱신을 거칠때마다 점점 작아지다가 사라진다. 이러한 문제를 기울기 소실(gradient vanishing) 문제라고 한다.

표준편차를 0.01로 바꾸어 다시 초기화해보면 출력값이 0.5 부근에 집중되어 출력되는데 이 경우 기울기 소실 문제는 발생하지 않으나, 다수의 뉴런이 같은 값을 출력하면 뉴런을 1개 쓰는 것과 차이가 없다. 즉, 표현력이 제한된다.

각 층의 활성화 값은 고루 분포되는게 좋은데, 값이 치우치면 기울기 소실 문제나 표현력 제한 문제로 학습이 잘 이뤄지지 않을 수 있기 때문이다.

Xavier 초기값은 사비에르와 요수아 벤지오가 권장하는 가중치 초기값이자 일반적인 딥러닝 프레임워크들이 표준으로 사용하는 초기값인데 앞 계층의 노드의 갯수가 n개이면 표준편차가 1/√ n인 분포를 사용한다.
Xavier 초기값을 넣어주고 실행을 한 결과를 보면 이전에 1이나 0.05의 표준편차를 사용하는 것 보다 더 고루 분포된다. 이 때, sigmoid 대신 tanh(쌍곡선) 함수를 활성화 함수로 사용하면 데이터가 더 고루 분포되는데, sigmoid와 tanh는 비슷한 형태를 띄고 있지만 tanh는 원점 대칭 함수이고, 활성화 함수로는 원점 대칭 함수가 더 바람직하다고 알려져 있다고 한다.

Xavier 초기값은 활성화 함수가 선형(sigmoid, tanh)이라는 전제로 효과적이다. ReLU를 사용할 때에는 He(히) 초기값을 사용하는데, 이는 앞 계층의 노드가 n개 일 때 표준편차가 √ (2/n) 인 정규분포를 사용한다.

## 배치 정규화
배치 정규화를 통해 각 층에 활성화 값이 적당히 퍼지게 할 수 있다.

배치 정규화로 얻을 수 있는 효과는 다음과 같다.
1. 학습을 빨리 진행할 수 있다.
2. 초기값에 크게 의존하지 않는다.
3. 오버피팅을 억제한다.

배치 정규화의 목적은 활성화 값을 적당히 분포 시키는 것으로 학습 시 미니배치를 단위로 정규화를 하는데, 데이터 분포의 평균이 0, 분산이 1이 되도록 정규화를 한다. 

![08](https://gityunjae.github.io/images/Chap06/08.png)

이러한 정규화 계층은 활성화 함수 앞 또는 뒤에 삽입한다.

또한, 배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대와 이동 변환을 수행하는데, 다음과 같은 식에서 학습을 통해 감마와 베타 값을 조정해 나간다.

![09](https://gityunjae.github.io/images/Chap06/09.png)

이 과정을 계산 그래프로 나타내면 다음과 같다.

![10](https://gityunjae.github.io/images/Chap06/10.png)

결론적으로 배치 정규화의 효과는 학습 속도가 높아지고 초기값의 영향을 덜 받는다는 것이다.

## 오버피팅
