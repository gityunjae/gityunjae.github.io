---
category: DLfS
layout: post
title: 6. 학습 관련 기술들
---
## 매개변수 갱신
신경망의 학습 목적은 손실 함수 값을 최대한 낮추는 매개변수를 찾는 것이다. 이는 다시 말하면 매개변수의 최적값을 찾는 것이고, 최적화(optimization)에 해당한다.

앞에서 미니 배치에 매개변수의 기울기로 미분을 사용하는 확률적 경사 하강법(SGD)을 알아보았다.

![01](https://gityunjae.github.io/images/Chap06/01.JPG)

SGD 방식은 단순하고 구현이 쉽지만 비효율적인 경우도 있다.
비등방성 함수(방향에 따라 성질, 기울기가 달라지는 함수)에서는 SGD의 탐색경로가 비효율적이다.

![02](https://gityunjae.github.io/images/Chap06/02.png)

이러한 단점을 개선해주는 방법으로 모멘텀, AdaGrad, Adam 방법이 있다.

### 모멘텀

![03](https://gityunjae.github.io/images/Chap06/03.JPG)

모멘텀은 v라는 속도에 해당하는 변수를 사용하는 방식으로 기울기 방향으로 힘을 받아 물체가 가속되는 물리 법칙을 적용한 방식이다.

![04](https://gityunjae.github.io/images/Chap06/04.png)

SGD와 비교해서 지그재그의 정도가 덜한것을 알 수 있는데, x축 방향 힘이 작지만 방향이 바뀌지 않기 때문에 점점 가속을 하기 때문이다.

### AdaGrad
신경망 학습에서 학습률은 너무 작으면 학습이 너무 오래 걸리고 너무 크면 값이 발산해버릴 수 있는, 상당히 중요한 파라미터이다.

학습률 감소(Learning rate decay)란 학습률을 크게 시작해서 점점 학습률을 줄여 나가는 방식인데, 기계학습에서 자주 쓰인다.

AdaGrad 방식에서는 학습률을 각각의 매개변수에 맞춤형으로 만들어주는 Adaptive 방식이다.

![05](https://gityunjae.github.io/images/Chap06/05.JPG)

식에서 h는 기존 기울기를 제곱해서 더해준 값이고, h의 제곱근의 역수를 곱해 학습률을 조정하는데, 매개변수 원소 중 많이 움직인 원소의 학습률이 낮아지는 방식이다.

![06](https://gityunjae.github.io/images/Chap06/06.png)

AdaGrad의 단점은 학습이 진행될수록 갱신강도가 약해져서 어느 순간부터 갱신이 안될 수 있다는 점인데, 이를 개선해서 지수이동평균 방식을 사용하여 과거의 기울이는 덜 반영하고 최근 정보는 더 크게 반영하는 방법을 RMSProp 방법이라고 한다.

### Adam
Adam 방식은 모멘텀과 AdaGrad를 융합한 모델로 효율적이고, 하이퍼 파라미터의 편향 보정 또한 진행된다고 한다. (논문을 보면 더 자세히 나온다고 하는데, 다음에 다른 교재를 공부할 때 기회가 되면 원리를 이해해보기로 하자.)

![07](https://gityunjae.github.io/images/Chap06/07.png)

SGD, 모멘텀, AdaGrad, Adam 모두 장단점이 있고, 각각의 문제에 따라, 하이퍼 파라미터에 따라 성능이 다르기 때문에 해결하려는 문제에 대해 여러가지로 시도해보는 것이 좋다.

## 가중치의 초기값
