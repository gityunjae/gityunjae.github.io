---
category: study
layout: post
title: BERT에 대해 알아보자
---
### BERT란?
2019년 NAACL에 게재된 "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"이라는 논문에서 제안된 모델이다. BERT는 Bidirectional Encoder Representations from Transformers의 줄임말로, 언어표현모델(language representation model), 정확히 말하면 학습 방법의 일종이다[1].

BERT 모델은 unlabeled text들로부터 deep bidirectional representation을 pre-train할 수 있고, 하나의 추가적인 output layer을 통해 sentence-level task부터 token-level tasks 등 다양한 nlp task에 적용할 수 있다. 이런 sentence-level task나 token-level task, 더 자세히는 question answering task, language inference task등 구체적으로 풀고 싶은 문제들을 downstream task라고 한다[3].

pre-trained language representation을 이러한 downstream task에 적용하는 방법은 feature-based approach와 fine-tuning approach으로 크게 두 가지가 있는데, ELMo의 경우 feature-based 방식을 사용하고 Generative Pre-trained Transformer(GPT) 모델의 경우 fine-tuning 방식을 사용한다. BERT는 feature-based 방식과 fine-tuning 방식 모두 사용 가능하지만 논문에서는 fine-tuning based approach를 중심으로 소개한다. 

![bert_model](https://gityunjae.github.io/images/BERT.PNG)
BERT는 transformer의 encoder를 활용한 모델인데(transformer에 대한 내용은 <a href="https://gityunjae.github.io/study/2020/09/07/TRANSFORMER/">TRANSFORMER에 대해 알아보자</a> 를 참고하자), transformer 모델에서는 attention head가 8개인 동일한 encoder layer 6층이 하나의 encoder로 사용되었던 반면 BERT는 base 모델 기준 attention head 12개, encoder layer 12층으로 이루어져있다.

BERT의 input은 WordPiece embedding을 사용하고, sequence(여기서 sequence는 input token sequence를 의미하는데 하나의 문장일수도 있고 둘 이상의 문장일수도 있다)의 시작부에 [CLS] 토큰을, 그리고 문장과 문장 사이에는 [SEP] 토큰을 추가해준다.

BERT는 크게 두 가지 task를 통해 pre-train이 이루어지는데, 첫 번째 task는 Masked Language Model(MLM) task이고, 두 번째 task는 next sentence prediction task이다.

### 1. Masked Language Model






출처: <br>
[1] <a href="https://medium.com/@jonathan_hui/nlp-bert-transformer-7f0ac397f524">nlp-bert-transformer</a><br>
[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br>
[3] <a href="https://velog.io/@nawnoes/Downstream-Task%EB%9E%80">Velog, Downstream task란?</a><br>
