---
category: study
layout: post
title: Batch normalization & Layer normalization
---

batch normalization과 layer normalization은 둘 다 값들이 심하게 차이나는 정도를 줄이기 위해 사용되는 방법인데, 
batch normalization은 각 feature의 평균과 분산으로 구해서 batch의 각 feature을 정규화 하는 방법이고, 
layer normalization은 각 input의 feature들에 대한 평균과 분산을 구해서 batch의 각 input을 정규화하는 방법이다[1].

<img src="https://gityunjae.github.io/images/bnln.png">

그래서 위와 같이 구한 평균과 분산을 사용해서 아래와 같은 방법으로 각각의 값들을 정규화 시켜준다[2].

<img src="https://gityunjae.github.io/images/bn.png">
<- batch normalization

<img src="https://gityunjae.github.io/images/ln.png">
<- layer normalization

<br><br>
출처: <br>
[1] <a href="https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/">Batch Norm vs Layer Norm</a><br>
[2] <a href="https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/">Weight Normalization and Layer Normalization Explained</a>
