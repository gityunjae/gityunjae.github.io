---
category: papers
layout: post
title: BART
---

제목: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

저자: Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

발행: 2020, Association for Computational Linguistics

BART는 seq2seq 모델 학습을 위해 denoising autoencoder 모델이다. Denoising AutoEncoder는 2008년 몬트리올 대학교의 Pascal Vincent, Yoshua Bengio 등이 발표한 "Extracting and Composing Robust Features with Denoising AutoEncoder"에서 제안된 개념으로, 자율학습을 이용하여 입력 데이터에 숨어있는 중요한 특징을 파악하는 용도로 개발되었다[1].

BART는 텍스트에 임의의 노이즈를 주고 원래의 text를 다시 복원하는 방식으로 학습되고, 트랜스포머 베이스 neural machine translation 구조를 사용한다.

BART는 이해를 필요로 하는 태스크에는 다 좋은 성능을 내지만, 특히 생성 태스크에 효과적이다. 

## 1. Introduction
Self-supervised 방법론 중 최근에 가장 성공적인 연구들은 모두 masked language model을 조금씩 변형한 모델이다. masked language model은 BERT에서 사용한 방법으로 특정 토큰들을 MASK 토큰으로 대체하여 해당 위치에 등장할 토큰들을 예측을 하는 학습방식이다. 

해당 논문에서 제안하는 것은 Bidirectional 그리고 Auto-Regressive Transformer를 융합한 모델을 기학습하는 방법인 BART이다. BART는 아주 다양한 end task에 적용할 수 있는 seq2seq 모델로 만든 denoising auto encoder이다. 학습은 텍스트에 노이즈를 추가해주고 원래 텍스트를 예측하는 방식으로 이루어지고, 트랜스포머 모델 구조를 기반으로 한다.

BART의 장점은 noise flexibility인데, 무작위의 부분을 가져다가 하나의 마스크 토큰으로 대체함으로써 원래 있던 단어들도 예측하고 몇 개의 토큰을 생성할지에 대해서도 학습이 이뤄진다. BART는 이해를 필요로 하는 문제에는 모두 좋은 성능으로 보이지만 그 중에서도 생성 문제에 파인튜닝했을 때 가장 효과적인데, 거의 RoBERTa급 성능을 보인다. BART는 파인튜닝에 관련해서도 새로운 관점들을 제시하는데, 기계번역 관련해서 transformer layer 위에 BART모델을 올려서 BART를 기학습된 타겟언어 모델로 사용할 수 있고, 이러한 접근을 통해서 성능이 많이 향상된다.

BART 모델의 장점과 요소들에 더 알아보기 위해 ablation 결과도 첨부했는데 이 연구를 통해 데이터와 최적화 파라미터 조정이 학습 objective를 설정하는 것 만큼이나 중요하다는 것 또한 알 수 있다.

## 2. Model
BART는 corrupted document를 original document로 맵핑하는 denoising autoencoder인데 seq2seq 모델과 bidirectional encoder를 사용했고 left-to-right autoregressive decoder를 사용했다(autoregressive decoder에 대해서는 GPT 논문을 읽으면서 더 찾아보기로 하자). 기학습을 위해 원래의 도큐멘트에 대한 negative log likelihood를 최적화한다.

### 2.1 Architecture
BART는 Transformer의 구조를 사용하면서 ReLU대신 GeLU 활성화 함수를 사용한다. base 모델로는 인코더 6층과 decoder 6층을 사용하고 large model로는 각각 12층을 사용했다. BERT와 다른 점은 각각의 decoder layer에서 encoder의 최종 hidden layer와의 cross-attention 연산을 한다는 것과 prediction 직전에 추가적인 feed-forward network을 사용하지 않는 것이다. BART는 같은 크기의 BERT보다 10%정도 파라미터가 더 많다.

### 2.2 Pre-training BART
BART는 

출처:<br>
[1] <a href="http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220891144201&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView">라온피플</a>, 네이버 블로그<br>
