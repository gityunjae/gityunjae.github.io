---
category: papers
layout: post
title: BART
---

제목: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

저자: Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

발행: 2020, Association for Computational Linguistics

BART는 seq2seq 모델 학습을 위해 denoising autoencoder 모델이다. Denoising AutoEncoder는 2008년 몬트리올 대학교의 Pascal Vincent, Yoshua Bengio 등이 발표한 "Extracting and Composing Robust Features with Denoising AutoEncoder"에서 제안된 개념으로, 자율학습을 이용하여 입력 데이터에 숨어있는 중요한 특징을 파악하는 용도로 개발되었다[1].

BART는 텍스트에 임의의 노이즈를 주고 원래의 text를 다시 복원하는 방식으로 학습되고, 트랜스포머 베이스 neural machine translation 구조를 사용한다.

BART는 이해를 필요로 하는 태스크에는 다 좋은 성능을 내지만, 특히 생성 태스크에 효과적이다. 

## 1. Introduction
Self-supervised 방법론 중 최근에 가장 성공적인 연구들은 모두 masked language model을 조금씩 변형한 모델이다. masked language model은 BERT에서 사용한 방법으로 특정 토큰들을 MASK 토큰으로 대체하여 해당 위치에 등장할 토큰들을 예측을 하는 학습방식이다. 

해당 논문에서 제안하는 것은 Bidirectional 그리고 Auto-Regressive Transformer를 융합한 모델을 기학습하는 방법인 BART이다. BART는 아주 다양한 end task에 적용할 수 있는 seq2seq 모델로 만든 denoising auto encoder이다. 학습은 텍스트에 노이즈를 추가해주고 원래 텍스트를 예측하는 방식으로 이루어지고, 트랜스포머 모델 구조를 기반으로 한다.

BART의 장점은 noise flexibility인데, 무작위의 부분을 가져다가 하나의 마스크 토큰으로 대체함으로써 원래 있던 단어들도 예측하고 몇 개의 토큰을 생성할지에 대해서도 학습이 이뤄진다. BART는 이해를 필요로 하는 문제에는 모두 좋은 성능으로 보이지만 그 중에서도 생성 문제에 파인튜닝했을 때 가장 효과적인데, 거의 RoBERTa급 성능을 보인다. BART는 파인튜닝에 관련해서도 새로운 관점들을 제시하는데, 기계번역 관련해서 transformer layer 위에 BART모델을 올려서 BART를 기학습된 타겟언어 모델로 사용할 수 있고, 이러한 접근을 통해서 성능이 많이 향상된다.

BART 모델의 장점과 요소들에 더 알아보기 위해 ablation 결과도 첨부했는데 이 연구를 통해 데이터와 최적화 파라미터 조정이 학습 objective를 설정하는 것 만큼이나 중요하다는 것 또한 알 수 있다.

## 2. Model
BART는 corrupted document를 original document로 맵핑하는 denoising autoencoder인데 seq2seq 모델과 bidirectional encoder를 사용했고 left-to-right autoregressive decoder를 사용했다(autoregressive decoder에 대해서는 GPT 논문을 읽으면서 더 찾아보기로 하자). 기학습을 위해 원래의 도큐멘트에 대한 negative log likelihood를 최적화한다.

### 2.1 Architecture
BART는 Transformer의 구조를 사용하면서 ReLU대신 GeLU 활성화 함수를 사용한다. base 모델로는 인코더 6층과 decoder 6층을 사용하고 large model로는 각각 12층을 사용했다. BERT와 다른 점은 각각의 decoder layer에서 encoder의 최종 hidden layer와의 cross-attention 연산을 한다는 것과 prediction 직전에 추가적인 feed-forward network을 사용하지 않는 것이다. BART는 같은 크기의 BERT보다 10%정도 파라미터가 더 많다.

### 2.2 Pre-training BART
BART는 먼저 document에 노이즈를 추가하고 reconstruction loss를 최적화 하는 방식으로 학습하는데 이 때 크로스 엔트로피로 loss를 계산한다. BART의 document corruption은 엄청 유동적인데 극단적으로 src 정보가 모두 없어지면 language model로 사용할 수 있다(이 부분은 잘 이해를 하지 못했다). 사용한 transformation들은 Token Masking(BERT와 동일), Token Deletion(랜덤한 토큰 삭제, 모델이 어느 위치에 input이 사라졌는지 예측), Text Infilling(푸아송 분포를 따르는 길이의 text span들을 하나의 MASK 토큰으로 대체, 모델은 몇개의 토큰이 없어졌는지 예측), Sentence Permutation(document를 문장으로 잘라서 랜덤한 순서로 shuffle), Document Rotation(토큰 하나를 정해서 그 토큰으로 document가 시작하도록 rotate, 모델은 document의 원래 시작점을 예측)이다.

## 3. Fine-tuning BART
BART로 학습된 representation은 downstream application을 위해 몇몇 방법으로 활용이 가능하다.

### 3.1 Sequence Classification Tasks
동일한 입력이 encoder와 decoder에 입력되어서 decoder token의 마지막 hidden state가 multi-class linear classifier의 인풋으로 사용되는데, BERT의 CLS토큰처럼 추가적인 토큰을 문장 맨 뒤에 붙여주었다.

### 3.2 Token Classification Tasks
SQUAD에서 answer endpoint를 분류하는 것 처럼 완전한 document를 각각 encoder와 decoder에 입력으로 넣어주고 decoder의 top hidden state를 각 단어의 representation으로 사용하고, 이 representation으로 토큰을 분류한다.

### 3.3 Sequence Generation Tasks
BART는 autoregressive decoder를 사용하기 때문에 abstractive question answering이나 요약등의 sequence 생성 태스크에 직접적으로 finetuning이 가능하다. 두 경우 모두 정보를 좀 조정해서 입력으로 넣어주는데 이 과정이 pre-training에서 denoising하는 방법과 유사하다. encoder의 입력은 input sequence이고 decoder에서는 출력을 autoregressive하게 생성한다.

### 3.4 Machine Translation
BART를 기계번역에도 사용해보는데 이전 연구들을 보면 pre-trained encoder와 결합하는 방식으로 기계번역을 하는 연구들이 있었지만 pre-trained language model을 디코더에서 사용하는 것은 어느정도 한계가 있다. 이 논문에서는 BART 모델 자체를 기학습된 디코더로 사용하고 encoder parameter 몇 개를 추가해주는 기계번역 방식을 시도하였다. 즉, BART의 인코더 임베딩 레이어에 새로 랜덤하게 초기화된 인코더를 꽂아주었다. 그리고 이 인코더를 두 단계로 학습시켰는데 두 경우 모두 BART 모델의 출력에 대한 cross-entropy loss를 역전파 하는 방식으로 학습하였고, 첫 단계에서는 BART의 파라미터를 대부분 고정하고 랜덤하게 초기화한 source encoder, BART의 인코더의 첫 계층에서의 self-attention input projection matrix와 BART positional embedding만 업데이트 해주었다. 두 번째 단계에서는 적은 횟수로 모든 파라미터를 학습하였다.

## 4. Comparing Pre-training Objects
BART는 다른 연구들보다 더 넓은 범주의 noising scheme을 지원한다. base model에서의 옵션들에 대해 비교해보았다.

### 4.1 Comparison Objectives
기학습 objective 중에서는 fair comparison이 있는데 이는 수행하기가 어렵기 때문에 새로운 objective를 제안하고자 했는데, 기학습 objective와 관계 없는 것들 중에서는 성능 향상을 위한 학습율고 계층 정규화를 제외하고는 최대한 차이가 없게 했다. 그래서 시도한 방법들은 Language Model(GPT같은 left to right Transformer language model), Permuted Langueage Model(XLNet같이 autoregressive generate하는 방식), Masked Language Model(BERT처럼 토큰 예측하는 방식), Multitask Masked Language Model(UniLM처럼 MLM에 self-attention mask을 추가한 방식), Masked Seq-to-Seq(MASS에서 착안한 마스킹 방식)을 사용했고, 이 중 Permuted LM, Masked LM, 그리고 Multitask Masked LM에 대해서는 two-stream attention을 사용하였다.

여러 실험을 해본 결과 task를 seq-to-seq 문제로 생각해서 인코더에 입력을 넣으면 디코더로 출력 예측하는 방식이 BART에 더 잘 맞는다는 것을 알 수 있었다.

성능은 perplexity로 평가하였다.

### 4.2 Tasks
SQuAD 태스크는 문단을 주고 질문을 주면 정답 span을 찾는 task이다. 

MNLI는 한 문장이 다른 한 문장과 함의 관계인지를 판단하는 task이다.

ELI5는 abstractive qa 데이터셋으로 질문과 이를 뒷받침하는 자료들로부터 정답을 생성하는 task이다.

XSum은 뉴스 요약 데이터셋으로 요약문은 상당히 추상적이다.

ConvAI2는 문맥과 페르소나가 주어졌을 때 담화 대답 생성 task이다.

CNN/DM은 뉴스 요약 데이터셋으로 요약문은 source 문장들과 높은 연관성이 있다.

### 4.3 Results
결과적으로 알 수 있는 경향들은 다음과 같다.

pre-training의 성능이 task마다 다르게 영향을 끼친다. 즉, task에 따라 효과적인 pre-training 방식이 다르다.

Token masking은 매우 중요하다.

Left-to-right pre-training은 생성 task의 성능을 개선시킨다.

양방향 인코더는 SQuAD에 매우 중요한 역할을 한다.

pre-training objective 말고도 성능에 영향을 끼치는 중요한 요소들이 많다.

Pure language model은 ELI5에서 가장 좋은 성능을 보인다.

BART가 여러가지 task에 대해 가장 일관적으로 좋은 성능을 보인다.

## 5. Large-scale Pre-training Experiments
최근 연구들에 따르면 pre-training 배치 크기를 확 키우면 downstream performance도 성능이 크게 향상된다고 한다. 이 관점에서 BART의 성능을 확인하기 위해 RoBERTa 모델과 같은 scale의 BART를 학습시켜보았다.

### 5.1 Experimental Setup
large model에서는 인코더와 디코더를 각각 12층씩 사용했고 hidden size는 1024로 설정했다. 배치 사이즈는 8000으로, 학습 step은 500000스텝으로 지정해주었다. 토큰화는 GPT-2와 동일한 byte-pair encoding을 사용하였고 text infilling과 sentence permutation 방식으로 학습하였다. 각 document에서 30%의 토큰을 마스킹해주었고 모든 문장들을 permute 해주었다. 이 permutation의 경우 크게 성능향상에 기여하지는 않았지만 더 큰 pre-train 모델에 적용하면 더 잘 학습하지 않을까 하는 생각에 넣어주었다. 그리고 마지막 10%의 step에서는 dropout을 하지 않았고, 학습 데이터로는 160GB의 뉴스, 책, 이야기, 그리고 웹 텍스트를 사용하였다.

### 5.2 Discriminative Tasks
SQuAD와 GLUE task를 보면 BART는 다른 모델들과 유사한 성능을 보였다.

### 5.3 Generation Tasks
생성 task들에서의 성능도 확인을 해보았는데 finetuning시 label smoothed cross entropy loss를 사용하였다.
생성시에는 beam size를 5로 두었고 중복된 trigram을 제거해주었다. 그리고 최소길이, 최대길이, 그리고 길이에 대한 패널티를 적용해주었다.

#### Summarization
CNN/DaillyMail, 그리고 XSum 데이터에 대한 성능을 다른 모델들과 비교해봤을 때 BART의 성능이 가장 높았다.

#### Dialogue
ConvAI에 대한 성능을 확인해봤을 때 F1 score와 Perplexity score 기준 BART의 성능이 가장 좋았다.

#### Abstractive QA
ELI5 데이터셋을 사용해서 모델이 긴 자유형식 답을 생성하는 task에 대한 성능을 확인해보았을 때 아직 부족하나 여타의 모델들보다 성능은 훨씬 개선되었다.

#### Translation
WMT16 Romanian-English 성능에서는 6계층 transformer source encoder와 BART를 사용했는데 back-translation 데이터가 없으면 덜 효과적이고 과적합이 되는 경향성을 띄었다.

## 6. Qualitative Analysis
BART는 요약 관련하여 매우 크게 성능 향상을 보였고, 추가적으로 BART의 성능에 대해 이해하기 위해 BART의 생성을 qualitative하게 분석하였다. WikiNews를 요약하는 task에 적용을 해보았는데, 가장 처음에 나오는 요약된 문장은 제거를 해주었다. 모델이 생성한 요약문은 매우 유창했지만 매우 추상적이었다. 이름이나 배경 지식은 잘 사용하는 것을 보였으나 이것이 source에 기반한 fact와는 거리가 있는 경우도 있었다. 하지만 결과적으로 BART pretraining이 natural language understanding과 generation을 모두 잘 학습했음을 알 수 있다.

## 7. Related Work
pretraining 연구 초기 단계에는 language model이 자주 쓰였다. 예시로 GPT, ELMo, BERT, UniLM, MASS, XLNet등이 있다.

몇몇 연구에서는 기계 번역의 성능을 향상하기 위한 pre-training 방법들을 연구했고, 가장 높은 성능은 source와 target 언어 모두에 대해 학습을 시킨 경우였지만 이러기 위해서는 사용할 모든 언어들에 대해 학습을 해야한다는 단점이 있다.
다른 연구에서는 인코더가 기학습된 representation을 사용해서 성능을 향상시킬 수 있다는 연구도 있었지만 encoder가 얻을 수 있는 정보가 한정적이다. 
해당 연구에서는 BART를 통해 기계번역 디코더를 향상시키는 방법에 대해 보였다.

## 8. Conclusions
결과적으로 BART는 document를 corrupt하고 원래 텍스트를 예측하는 pretraining 방식이고 분류 문제에서는 RoBERRTa와 유사한 성능을 보이고 생성 task에서는 SoTA 성능을 보인다. 이후 추가적인 연구로는 pretraining시 document를 corrupt하는 새로운 방법들에 대해 연구해야 할 것이다.

출처:<br>
[1] <a href="http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220891144201&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView">라온피플</a>, 네이버 블로그<br>
