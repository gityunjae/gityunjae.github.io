---
category: papers
layout: post
title: BART
---

제목: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension

저자: Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer

발행: 2020, Association for Computational Linguistics

BART는 seq2seq 모델 학습을 위해 denoising autoencoder 모델이다. Denoising AutoEncoder는 2008년 몬트리올 대학교의 Pascal Vincent, Yoshua Bengio 등이 발표한 "Extracting and Composing Robust Features with Denoising AutoEncoder"에서 제안된 개념으로, 자율학습을 이용하여 입력 데이터에 숨어있는 중요한 특징을 파악하는 용도로 개발되었다[1].

BART는 텍스트에 임의의 노이즈를 주고 원래의 text를 다시 복원하는 방식으로 학습되고, 트랜스포머 베이스 neural machine translation 구조를 사용한다.

BART는 이해를 필요로 하는 태스크에는 다 좋은 성능을 내지만, 특히 생성 태스크에 효과적이다. 

# 1. Introduction


출처:<br>
[1] <a href="http://blog.naver.com/PostView.nhn?blogId=laonple&logNo=220891144201&categoryNo=0&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=postView">라온피플</a>, 네이버 블로그<br>
